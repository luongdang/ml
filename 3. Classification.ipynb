{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "\n",
    "## Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COL_NAMES': ['label', 'data'],\n",
       " 'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([ 0.,  0.,  0., ...,  9.,  9.,  9.])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` key contains the features of the image, which is a 28 x 28 (784px) bitmap of 0 and 1. The `target` key contains the labels, i.e. the digit that represent the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (70000, 784) y shape = (70000,)\n"
     ]
    }
   ],
   "source": [
    "x,y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(\"x shape =\", x.shape, \"y shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `imshow` to convert a array of pixels to an actual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11552e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = x[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above looks like a \"5\" and indeed it is a 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[36000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Always create a test and trainset\n",
    "\n",
    "Fortunately, SKLearn has done that for us. The first 60000 data points are the trainset, containing a roughly equal number of images of digits 0 - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# Shuffle the sets so the algorithm do not get several same\n",
    "# digits in a row which could degrade performance\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a binary classifier\n",
    "Let's simplify the problem by trying to identify only one digit, for example, 5. This is an example of a binary classifier, capable of distinguishing just two classes, 5 and not-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # True for all 5s, False for everything else\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=10, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train it using the Stochastic Gradient Descent classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42, max_iter=10)\n",
    "sgd_clf.fit(x_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measure\n",
    "Evaluating a classifier is often significantly trickier than a regressor. Let's start with an old tool: the cross validator\n",
    "\n",
    "## Measuring accuracy using cross validation\n",
    "Recall that K-fold cross-validation means splitting the training set into K subsets (i.e. folds) and making prediction and scoring them on fold. Let use `K = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.957 ,  0.9636,  0.9653])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 95% score looks good, but there's more than meet the eyes. Let's write a very dumb classifier that classifies every image into the \"not-5\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0961 ,  0.1003 ,  0.09975])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class NeverClassifier(BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        pass\n",
    "    def predict(self, x):\n",
    "        return np.zeros((len(x), 1), dtype=bool)\n",
    "    \n",
    "never_5_clf = NeverClassifier()\n",
    "cross_val_score(never_5_clf, x_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 90% accuracy for a garbage classifier! That's because only 10% of the training set is 5. The other 90% is not-5. So even with a garbage classifier, you are still corect 90% of the time. This demonstrates that **accuracy is often not the preferred performance measure** for classifiers, especially when you are dealing with skwed datasets, when some classes appear much more frequently than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "A confusion matrix counts the number of times an instance of class A was misidentifier as an instance of class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look into the 5th row and 3rd column of the matrix.\n",
    "\n",
    "The `cross_val_predict` function divides the set into smaller subsets and make clean predictions on each subset. Clean meaning the prediction is made by a model that never saw the data during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to intepret the confusion matrix:\n",
    "* The first row and first column contains the not-5 (the False/Negative class)\n",
    "* The second row and second column contains the is-5 (the True/Positive class)\n",
    "* 53,814 images are not-5 and correctly classified so (true negatives)\n",
    "* 765 images are not-5 but incorrectly classified as is-5 (false positives)\n",
    "* 1,517 are 5 but wrongly classified as not-5 (true positives)\n",
    "* 3,904 are 5 and correctly classified as such (false negatives)\n",
    "\n",
    "A perfect classifier would have only the diagonals and 0 for all other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53814,   765],\n",
       "       [ 1517,  3904]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may prefer some more concise metrics:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{precision} &= \\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Positives}} \\\\\\\\\n",
    "\\text{recall}    &= \\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* precision alone is not enough because if you make one only one TP and no FP have perfect precision\n",
    "* recall (a.k.a sensitivity or true positive rate) is the ratio of positive instances that are correctly detected by the classifier\n",
    "\n",
    "You do not have to create your own implementation of precision and recall. SKLearn already provides them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83615335189548079"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72016233167312305"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier doesn't look as good as it was. It only detects a 5 correctly 84% of the times, and only found 72% of the 5s.\n",
    "\n",
    "It's often convenient to combine the precision and recall into a single metric called $F_1$ score if you need a metric to compare 2 classifiers. $F_1$ is the harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\div \\left( \\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}} \\right)\n",
    "$$\n",
    "\n",
    "Or call the `f1_score()` function from SKLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77383548067393448"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F_1$ score tends to favour classifiers with similar precision and recall. Sometimes you may prefer one of the other. For example:\n",
    "* If you train a video classifier to filter videos that are safe for kids, you would prefer a high-precision (fewer \"bad\" videos); a low-recall rate (more \"good\" videos marked as unsafe) would be acceptable\n",
    "* If you train a video classifier to catch shop-lifter, you would accept a low-precision (more shoppers tagged as suspects) to favour high-recall (more actual thieves will be tagged)\n",
    "\n",
    "Unfortunately, you can't have it both way. Increasing one reduces the other. This is called the *precision/recall tradeoff*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
